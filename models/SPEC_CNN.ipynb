{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwlLi21f2h3Ecdibo2hfqH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cengizmehmet/BenchmarkNets/blob/main/models/SPEC_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CONVOLUTIONAL NEURAL NETWORKS**"
      ],
      "metadata": {
        "id": "3WclNpRdpYDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepared by Mehmet CENGIZ**\n",
        "\n",
        "ORCID: 0000-0003-4972-167X\n",
        "\n",
        "This script is built to create CNNs trained in the SPEC CPU2017 dataset. The source of the dataset can be found on the website of Standard Performance Evaluation Corporation ([SPEC](https://www.spec.org/cpu2017/results/)). You can access the modified dataset in line with our requirements from the [data](https://github.com/cengizmehmet/BenchmarkNets/tree/main/data) folder of this repository. Those who will use this script is free to modify this adhering to their needs."
      ],
      "metadata": {
        "id": "YkRzSwdwpsFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "riNhaheHO8GT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NECESSARY DEPENDENCIES AND LIBRARIES**"
      ],
      "metadata": {
        "id": "kG3mYApAgWAo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI2zK1Tn2aH5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.random import seed\n",
        "from typing import Tuple, List\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, Flatten\n",
        "from sklearn.metrics import *\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import gaussian_kde"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Versions:** This information is the library versions in Google Colab when the models were first designed (around the end of 2022). Version differences may occur due to time and programming environment changes.\n",
        "\n",
        "* tensorflow: 2.12.0\n",
        "* pandas: 1.5.3\n",
        "* numpy: 1.22.4\n",
        "* seaborn: 0.12.2\n",
        "* sklearn-pandas: 2.2.0\n",
        "\n"
      ],
      "metadata": {
        "id": "UizXAgivo-JB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NECESSARY FUNCTIONS**"
      ],
      "metadata": {
        "id": "A_RihQftrkvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method Name:** CNN\n",
        "\n",
        "**Parameters:** Tuple, list, list, list, list, list\n",
        "\n",
        "**Return:** keras.Sequential\n",
        "\n",
        "This function builds Convolutional Neural Networks. *input_shape* defines the shape of the input neuron. *filters_in_convs* holds the information of the number of convolutional layers and kernels in each layer. *kernel_size* is the shape of kernels. *neurons_in_denses* holds the information of the number of layers and neurons in each dense layer. *activations* holds the activation functions of each layer.\n",
        "\n",
        "As this case is a regression problem, the output layer is defined with one neuron with the activation function of linear. Finally, this function returns an CNN."
      ],
      "metadata": {
        "id": "V3Fxyh0AsAQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN(input_shape: Tuple, filters_in_convs: List, kernels_sizes: List, strides_sizes: List,\n",
        "                      neurons_in_denses: List, activations: List) -> keras.Sequential:\n",
        "  count = len(filters_in_convs)\n",
        "  activation_count = 1\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(filters = filters_in_convs[0], kernel_size = kernels_sizes[0], strides = strides_sizes[0],\n",
        "                   activation=activations[0], input_shape = input_shape))  \n",
        "  for i in range(1, count):\n",
        "    model.add(Conv1D(filters = filters_in_convs[i], kernel_size = kernels_sizes[i],\n",
        "                     strides = strides_sizes[i], activation = activations[activation_count]))\n",
        "  model.add(Flatten())\n",
        "  count = len(neurons_in_denses)\n",
        "  model.add(Dense(units=neurons_in_denses[0], activation = activations[activation_count]))  \n",
        "  for i in range(1, count):\n",
        "    activation_count += 1\n",
        "    model.add(Dense(units=neurons_in_denses[i], activation = activations[activation_count]))\n",
        "  model.add(Dense(units=1, activation='linear'))\n",
        "  return model"
      ],
      "metadata": {
        "id": "4LBHtzjL4JWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method Name:** correlation\n",
        "\n",
        "**Parameters:** str, float, bool, Tuple, isCbar, save\n",
        "\n",
        "**Return:** set\n",
        "\n",
        "This function calculates the correlation of each column and return the columns that have more than *abs(threshold)* correlation value.\n",
        "\n",
        "* *corr_method* holds the correlation method. The correlation method may be kendall, spearman, and pearson. The default value is 'kendall'.\n",
        "* *threshold* defines the acceptable correlation range. The default value is 0.7.\n",
        "* *show* allows the matrix to be drawn or not. The default value is True\n",
        "* *fig_dims* holds the size of the figure of the correlation matrix. The default value is (12, 8).\n",
        "* *iscbar* allow the colour bar to be added or not. In some cases the figure of the correlation matrix does not fit the screen and the colour bar overlaps the matrix. The *isCbar* parameter exists to handle overlaps. The default values is True.\n",
        "* *save* allows the save the figure of the matrix. The default values is False."
      ],
      "metadata": {
        "id": "wbpF8fHO6IeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def correlation(corr_method: str = 'kendall', threshold: float = 0.7, show: bool = True, fig_dims: Tuple = (12, 8),\n",
        "                isCbar: bool = True, save: bool = False) -> set:\n",
        "  corr_features = set()\n",
        "  corr_matrix = dataset.corr(method = corr_method)\n",
        "  for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i):\n",
        "      if abs(corr_matrix.iloc[i, j]) > threshold:\n",
        "        colname = corr_matrix.columns[i]\n",
        "        corr_features.add(colname)\n",
        "  if show:\n",
        "    fig, ax = plt.subplots(figsize = fig_dims)\n",
        "    sns.heatmap(corr_matrix, ax = ax, annot = True, cmap = plt.cm.CMRmap_r, cbar = isCbar)\n",
        "    if save:\n",
        "        plt.savefig(\"png_format.png\", dpi = 300, format = \"png\")\n",
        "        plt.savefig(\"tiff_format.png\", dpi = 300, format = \"tiff\")\n",
        "    plt.show()\n",
        "  return corr_features"
      ],
      "metadata": {
        "id": "TQTCS5Ld4Dok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method Name:** split_dataset\n",
        "\n",
        "**Parameters:** pd.DataFrame, float, bool, str\n",
        "\n",
        "**Return:** Tuple[list, list, list, list, list, list]\n",
        "\n",
        "This function splits the dataset and returns each part.\n",
        "\n",
        "* *dataset* is the dataset to be split.\n",
        "* *target_column* holds the name of the target column.\n",
        "* *split_ratio* defines the ratio of the training and test sets. The default value is 0.8 which means 80% of the dataset is split as the training dataset.\n",
        "* *shuffle* allows the dataset to be shuffled. The default value is True.\n",
        "\n",
        "This function returns:\n",
        "* The training dataset\n",
        "* The test dataset\n",
        "* The independent columns of the training dataset\n",
        "* The target column of the training dataset\n",
        "* The independent columns of the test dataset\n",
        "* The target column of the test dataset\n",
        "* All independent columns as one list\n",
        "* The target column as one list"
      ],
      "metadata": {
        "id": "e48_iLHTAau6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(dataset: pd.DataFrame, target_column: str, split_ratio: float = 0.8, shuffle: bool = True) -> Tuple[list, list, list, list, list, list]:  \n",
        "  target_index = dataset.columns.get_loc(target_column)\n",
        "  data = np.array(dataset)\n",
        "  rows, columns = data.shape\n",
        "  if shuffle:\n",
        "    np.random.shuffle(data)\n",
        "  train_size = int(split_ratio * rows)\n",
        "  test_size = rows - train_size\n",
        "  train = data[:train_size].T\n",
        "  y_train = train[target_index]\n",
        "  X_train = np.delete(train.T, obj = target_index, axis = 1)\n",
        "  test = data[train_size:].T\n",
        "  y_test = test[target_index]\n",
        "  X_test = np.delete(test.T, obj = target_index, axis = 1)\n",
        "  data = data.T\n",
        "  y = data[target_index]\n",
        "  X = np.delete(data.T, obj = target_index, axis = 1)\n",
        "  return train.T, X_train, y_train, test.T, X_test, y_test, X, y"
      ],
      "metadata": {
        "id": "B5vrPVlWAYNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method Name:** factorize_columns\n",
        "\n",
        "**Parameters:** pd.DataFrame, str\n",
        "\n",
        "**Return:** pd.DataFrame, dictionary\n",
        "\n",
        "This function converts values of the columns to type of the target column. The details of the factorization process is [here](https://pandas.pydata.org/docs/reference/api/pandas.factorize.html).\n",
        "\n",
        "* *dataset* is the dataset to be factorized.\n",
        "* *target* holds the name of the target column.\n",
        "\n",
        "This function return both the converted dataset and labels of each converted value."
      ],
      "metadata": {
        "id": "_oSmP0zgZt1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def factorize_columns(dataset: pd.DataFrame, target: str) -> Tuple[pd.DataFrame, dict]:\n",
        "  all_labels = {}\n",
        "  for column in dataset.columns:\n",
        "    if dataset[column].dtypes != dataset[target].dtypes:\n",
        "      dataset[column], labels = pd.factorize(dataset[column])\n",
        "      all_labels[column] = labels\n",
        "  return dataset, all_labels"
      ],
      "metadata": {
        "id": "6kwV9pgt_O45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PREPROCESS**"
      ],
      "metadata": {
        "id": "njwXOTaZ2pNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variables of the dataset:"
      ],
      "metadata": {
        "id": "4_xIplD32yeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'https://raw.githubusercontent.com/cengizmehmet/BenchmarkNets/main/data/SPEC2017_modified.csv'\n",
        "dataset = pd.read_csv(path)\n",
        "target = \"Base_Result\" #Target column"
      ],
      "metadata": {
        "id": "n8rh2yXVoy5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attributes = dataset.columns\n",
        "rows, columns = dataset.shape\n",
        "dtypes = dataset.dtypes\n",
        "\n",
        "print(attributes)\n",
        "print(\"-----\")\n",
        "print(\"Columns: \" + str(rows) + \"\\n\" + \"Rows: \" + str(columns))\n",
        "print(\"-----\")\n",
        "print(dtypes)"
      ],
      "metadata": {
        "id": "af9eDJh1xWYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variables related to the correlation analysis:"
      ],
      "metadata": {
        "id": "V3ZxYks223W8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_method = 'kendall'\n",
        "threshold = 0.7 #It is accepted that higher and lower values than 0.7 and -0.7 respectively point out the high correlation\n",
        "show = True #This defines whether the correlation matrix is drawn\n",
        "fig_dims = (12, 8) #This defines the size of the figure in case draw is True\n",
        "isCbar = False #This defines whether the colour bar is added as legend\n",
        "save = False #This defines whether the correlation matrix is saved"
      ],
      "metadata": {
        "id": "4JJIPQ4B2uQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following line, those two columns are dropped intentionally. The dataset contains two target column candidates: Peak Result and Base Result. Since we pick Base Result as the target column, Peak Result is dropped. The Disclosure column contains the HTML output of the benchmark of systems. Basically, it involves the same information as other columns."
      ],
      "metadata": {
        "id": "hNG4Otgvz77N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.drop(['Peak_Result'], axis = 1)\n",
        "dataset = dataset.drop(['Disclosures'], axis = 1)"
      ],
      "metadata": {
        "id": "TPctY8Bpz7oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, labels = factorize_columns(dataset, target)"
      ],
      "metadata": {
        "id": "hvgTQnwbhysA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlated_features = correlation(corr_method = corr_method, threshold = threshold, show = show,\n",
        "                                  fig_dims = fig_dims, isCbar = isCbar, save = save)"
      ],
      "metadata": {
        "id": "yB4pOMeqhyo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(correlated_features)"
      ],
      "metadata": {
        "id": "t6HakDmehymA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.drop(correlated_features, axis = 1) #Correlated column are dropped"
      ],
      "metadata": {
        "id": "fTeyx3rvhyhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After dropping the correlated columns, their label also must be dropped."
      ],
      "metadata": {
        "id": "pcqSI9-hrEHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for label in correlated_features:\n",
        "    labels.pop(label, None)"
      ],
      "metadata": {
        "id": "Jf7TL5fvrDrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to provide controlled randomness, seeds are used."
      ],
      "metadata": {
        "id": "7BH92fmqr6xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed(1)"
      ],
      "metadata": {
        "id": "S1jaefA0kumZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variables related to the split dataset:"
      ],
      "metadata": {
        "id": "gewMXmFivg6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m, n = dataset.shape\n",
        "split = 0.8\n",
        "shuffle = True"
      ],
      "metadata": {
        "id": "zpSe9biZkujw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, X_train, y_train, test, X_test, y_test, X, y = split_dataset(dataset = dataset, target_column = target, split_ratio = split, shuffle = shuffle)"
      ],
      "metadata": {
        "id": "QjhKzRpcu2M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MODEL**"
      ],
      "metadata": {
        "id": "n-puwWujHJLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Initialisation**"
      ],
      "metadata": {
        "id": "Cg5KjgUYHhq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The parameters of the MLP:"
      ],
      "metadata": {
        "id": "2fGGF-1vZRuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = ((n - 1), 1)\n",
        "filters_in_convs = [512, 128]\n",
        "kernels_sizes = [3] * len(filters_in_convs)\n",
        "strides_sizes = [1] * len(filters_in_convs)\n",
        "neurons_in_denses = [512, 256, 128, 64, 32, 16]\n",
        "activations = ['relu'] * (len(filters_in_convs) + len(neurons_in_denses))"
      ],
      "metadata": {
        "id": "nMI5HfBV2vaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN(input_shape, filters_in_convs, kernels_sizes, strides_sizes, neurons_in_denses, activations)"
      ],
      "metadata": {
        "id": "-9ApxIav2vXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "p0WLM1gP2vUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training**"
      ],
      "metadata": {
        "id": "CyFtJ4JdH1k-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training parameters:"
      ],
      "metadata": {
        "id": "zt85YG-WRbr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = \"mean_absolute_error\"\n",
        "opt = \"adam\" \n",
        "learning_rate = 0.001\n",
        "metrics = [\"mean_absolute_error\", \"mean_squared_error\", \"mean_absolute_percentage_error\", \"mean_squared_logarithmic_error\", \"logcosh\"]\n",
        "epochs = 5\n",
        "batch_size = 10\n",
        "validation_split = 0.2\n",
        "verbose = 1 #It may be 0 or 1"
      ],
      "metadata": {
        "id": "FCe5tALlRfik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "        loss = loss,\n",
        "        optimizer = opt,\n",
        "        metrics = metrics\n",
        "        )"
      ],
      "metadata": {
        "id": "1_nb_7eTR52T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "      X_train,\n",
        "      y_train,\n",
        "      epochs = epochs,\n",
        "      batch_size = batch_size,\n",
        "      verbose = verbose,\n",
        "      validation_split = validation_split\n",
        "      )"
      ],
      "metadata": {
        "id": "YJepBDktT_Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance of the training phase:"
      ],
      "metadata": {
        "id": "VCTEBDT7U7Tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This line allow to store training results in a dictionary. \n",
        "results_dict = {}\n",
        "for key in history.history.keys():\n",
        "  results_dict[key] = history.history[key]"
      ],
      "metadata": {
        "id": "F72rkEjZcaz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To present average performance of the model\n",
        "for key in results_dict:\n",
        "  print(str(key) + '= ' + str(sum(results_dict[key]) / len(results_dict[key])))"
      ],
      "metadata": {
        "id": "KxK17PfucIhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the results of the training phase:"
      ],
      "metadata": {
        "id": "40UIXJ9eem13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "size = int(len(results_dict) / 2)\n",
        "keys = list(results_dict.keys())\n",
        "eps = range(1, epochs + 1)\n",
        "for i in range(size):\n",
        "  plt.plot(eps, results_dict[list(results_dict.keys())[i]], 'b', label = list(results_dict.keys())[i])\n",
        "  plt.plot(eps, results_dict[list(results_dict.keys())[i + 6]], 'r', label = list(results_dict.keys())[i + 6])\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel(list(results_dict.keys())[i])\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "wR_hKxjEZpK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Test**"
      ],
      "metadata": {
        "id": "x8kFBMKSUTCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(X_test)"
      ],
      "metadata": {
        "id": "zBn6Mmr0UQWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evalution of the test:"
      ],
      "metadata": {
        "id": "u2-qzv9iUZqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Metrics\n",
        "  #R2\n",
        "r2_value = r2_score(y_test, preds)\n",
        "print(\"R2 = \" + str(r2_value))\n",
        "\n",
        "  #MSE\n",
        "mse = mean_squared_error(y_test, preds, squared = True)\n",
        "print(\"MSE = \" + str(mse))\n",
        "\n",
        "  #RMSE\n",
        "rmse = mean_squared_error(y_test, preds, squared = False)\n",
        "print(\"RMSE = \" + str(rmse))\n",
        "\n",
        "  #MAE\n",
        "mae = mean_absolute_error(y_test, preds)\n",
        "print(\"MAE = \" + str(mae))\n",
        "\n",
        "  #Explained Variance Score\n",
        "evs = explained_variance_score(y_test, preds)\n",
        "print(\"EVS = \" + str(evs))\n",
        "\n",
        "  #Mean Pinball Loss\n",
        "mpl = mean_pinball_loss(y_test, preds)\n",
        "print(\"MPL = \" + str(mpl))"
      ],
      "metadata": {
        "id": "wQDnb6fvUbk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the results of the test phase:"
      ],
      "metadata": {
        "id": "VCkyCC9wwyVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Scatter plot\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "ax.scatter(y_test, preds, c='crimson')\n",
        "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'b-')\n",
        "ax.set_xlabel('Actuals')\n",
        "ax.set_ylabel('Predictions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VmZRL9tzT5dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Heat map\n",
        "xy = np.vstack([y_test, preds.flatten()])\n",
        "z = gaussian_kde(xy)(xy)\n",
        "idx = z.argsort()\n",
        "x, y, z = y_test[idx], preds[idx], z[idx]\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "ax.set_xlabel('Actuals')\n",
        "ax.set_ylabel('Predictions')\n",
        "cax = ax.scatter(x, y, c=z, s=50)\n",
        "fig.colorbar(cax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ms98o5TLxI7v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}